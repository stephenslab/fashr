---
title: "Basic Usage of fashr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Basics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup}
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)
library(fashr)
library(ggplot2)
```

```{r}
# Plot power against FDR
power_versus_fdr <- function(fdr_result, true_indices, fdr_vec = seq(0.01,0.99,by = 0.01), plot = TRUE){
  power <- numeric(length(fdr_vec))
  for (i in 1:length(fdr_vec)){
    significant_indices <- fdr_result$index[fdr_result$FDR < fdr_vec[i]]
    power[i] <- length(intersect(significant_indices, true_indices))/length(true_indices)
  }
  result <- data.frame(fdr = fdr_vec, power = power)
  if (plot){
    plot(fdr_vec, power, type = "l", xlab = "FDR", ylab = "Power")
  }
  return(result)
}

# Plot calibration of FDR
calibration_fdr <- function(fdr_result, true_indices, fdr_vec = seq(0.01,0.99,by = 0.01), plot = TRUE){
  true_discovery_rate <- numeric(length(fdr_vec))
  for (i in 1:length(fdr_vec)){
    significant_indices <- fdr_result$index[fdr_result$FDR < fdr_vec[i]]
    true_discovery_rate[i] <- length(intersect(significant_indices, true_indices))/length(significant_indices)
  }
  result <- data.frame(fdr = fdr_vec, Tfdr = (1 - true_discovery_rate))
  if (plot){
    plot(Tfdr ~ fdr_vec, type = "l", xlab = "nominal FDR", ylab = "observed FDR", data = result)
    lines(c(0,1), c(0,1), col = "red", lty = 2)
  }
  return(result)
}
```

## Overview

Suppose we have $N$ sets of eQTLs. The effect size estimate for the $i$th eQTL at time $t_{ir}$, where $r \in \{1, \ldots, R_i\}$, is denoted by $\hat{\beta}_i(t_{ir})$.
Given the true effect $\beta_i(t_{ir})$, the observed estimate is assumed to follow
$$
\hat{\beta}_i(t_{ir}) \mid \beta_i(t_{ir}) \overset{ind}{\sim} \mathcal{N}(\beta_i(t_{ir}), s_{ir}^2),
$$
where $s_{ir}$ denotes the standard error at time $t_{ir}$.
Let $\boldsymbol{\hat{\beta}}_i = [\hat{\beta}_i(t_{i1}), \ldots, \hat{\beta}_i(t_{iR_i})] \in \mathbb{R}^{R_i}$ and $\boldsymbol{s}_i = [s_{i1}, \ldots, s_{iR_i}] \in \mathbb{R}^{R_i}$ denote the vectors of effect size estimates and standard errors for the $i$th eQTL, respectively. 

The goal of `fashr` is to perform functional adaptive shrinkage (FASH) for inferring the posterior distribution of the effect size function $\beta_i(t)$, given the observed data $\boldsymbol{\hat{\beta}}_i$ and $\boldsymbol{s}_i$.

FASH assumes that all $\beta_i(t)$ are i.i.d. draws from a common prior $g_\beta$. 
The prior $g_\beta$ has the form of a finite mixture of Gaussian processes (GPs):
$$
g_\beta \mid \boldsymbol{\pi} = \sum_{k=0}^{K} \pi_k \, \text{GP}_k.
$$
Each GP component $\text{GP}_k$ is specified via the differential equation: if $\beta(t) \sim \text{GP}_k$, then
$$
L\beta(t) = \sigma_k W(t),
$$
where $W(t)$ is a Gaussian white noise process and $L$ is a known $p$th-order linear differential operator. The standard deviation parameter $\sigma_k$ determines how much $\beta(t)$ can deviate from the base model, defined as the null space $S_0 = \text{Null}\{L\}$.

Given a grid of $\{\sigma_1, ..., \sigma_K\}$, the prior mixing weights $\hat{\boldsymbol{\pi}}$ are estimated by maximizing the marginal likelihood of the observed effect size estimates:
$$
\hat{\boldsymbol{\pi}} = (\hat{\pi}_1, ..., \hat{\pi}_K) = \arg\max_{\boldsymbol{\pi}} \sum_{i=1}^{N} \log\left(\sum_{k=0}^{K} \pi_k \mathbf{L}_{ik}\right),
$$
where $\mathbf{L}_{ik}$ denotes the marginal likelihood of $\boldsymbol{\hat{\beta}}_i$ under the $k$th GP component.
Based on the estimated prior $\hat{g}_\beta$, the posterior distribution for $\beta_i(t)$ can be computed as:
$$
p(\beta_i(t) \mid \boldsymbol{\hat{\beta}}_i, \hat{\boldsymbol{\pi}}) = \sum_{k=0}^{K} \tilde{\pi}_k \, p_k(\beta_i(t) \mid \boldsymbol{\hat{\beta}}_i),
$$
where $p_k(\beta_i(t) \mid \boldsymbol{\hat{\beta}}_i)$ is the posterior distribution under the $k$th GP component.

In the following section, we illustrate this with simulated data.

## Setup

We consider effect size estimates for $N = 100$ eQTLs measured from day $t = 1$ to day $t = 16$:

- There are 50 eQTLs that are non-dynamic, that is, their effect sizes remain constant over time (Category A).
- There are 30 eQTLs with linear dynamics, that is, their effect sizes change linearly over time (Category B).
- There are 20 eQTLs with nonlinear dynamics, that is, their effect sizes change nonlinearly over time (Category C).

For each eQTL at each time point, the standard error $s_{ir}$ is randomly drawn from $\{0.05, 0.1, 0.2\}$.


```{r, fig.height=8, fig.width=7}
set.seed(1)
N <- 100
propA <- 0.5; propB <- 0.3; propC <- 0.2
sigma_vec <- c(0.05, 0.1, 0.2)

sizeA <- N * propA
data_sim_list_A <- lapply(1:sizeA, function(i) simulate_process(sd_poly = 0.2, type = "nondynamic", sd = sigma_vec, normalize = TRUE))

sizeB <- N * propB
if(sizeB > 0){
data_sim_list_B <- lapply(1:sizeB, function(i) simulate_process(sd_poly = 1, type = "linear", sd = sigma_vec, normalize = TRUE))

}else{
  data_sim_list_B <- list()
}

sizeC <- N * propC
data_sim_list_C <- lapply(1:sizeC, function(i) simulate_process(sd_poly = 0, type = "nonlinear", sd = sigma_vec, sd_fun = 1, p = 1, normalize = TRUE))

datasets <- c(data_sim_list_A, data_sim_list_B, data_sim_list_C)
labels <- c(rep("A", sizeA), rep("B", sizeB), rep("C", sizeC))
indices_A <- 1:sizeA
indices_B <- (sizeA + 1):(sizeA + sizeB)
indices_C <- (sizeA + sizeB + 1):(sizeA + sizeB + sizeC)

dataset_labels <- rep(as.character(NA),100)
dataset_labels[indices_A] <- paste0("A",seq(1,length(indices_A)))
dataset_labels[indices_B] <- paste0("B",seq(1,length(indices_B)))
dataset_labels[indices_C] <- paste0("C",seq(1,length(indices_C)))
names(datasets) <- dataset_labels

par(mfrow = c(3, 3))
for(i in indices_A[1:3]){
  plot(datasets[[i]]$x, datasets[[i]]$y, type = "p", col = "black", lwd = 2, xlab = "time", ylab = "effect", ylim = c(-1.5, 1.5), main = paste("Category A: ", i))
  lines(datasets[[i]]$x, datasets[[i]]$truef, col = "red", lwd = 1)
}

for(i in indices_B[1:3]){
  plot(datasets[[i]]$x, datasets[[i]]$y, type = "p", col = "black", lwd = 2, xlab = "time", ylab = "effect", ylim = c(-1.5, 1.5), main = paste("Category B: ", i))
  lines(datasets[[i]]$x, datasets[[i]]$truef, col = "red", lwd = 1)
}

for(i in indices_C[1:3]){
  plot(datasets[[i]]$x, datasets[[i]]$y, type = "p", col = "black", lwd = 2, xlab = "time", ylab = "effect", ylim = c(-1.5, 1.5), main = paste("Category C: ", i))
  lines(datasets[[i]]$x, datasets[[i]]$truef, col = "red", lwd = 1)
}

par(mfrow = c(1, 1))
```

Let's take a look at the data structure:

```{r}
length(datasets)
```

This is the first dataset:

```{r}
datasets[[1]]
```

Take a look at the true label of the datasets:

```{r}
table(labels)
```

## Fitting FASH

The main function for fitting FASH is `fash()`. 
By default, users should provide a list of datasets (`data_list`), and specify the column names for the effect size (`Y`), its standard deviation (`S`), and the time variable (`smooth_var`).

Computation can be parallelized by specifying the number of cores (`num_cores`).  
Reducing the number of basis functions (`num_basis`) can also improve computational speed.

The function `fash()` sequentially performs the following key steps:

1. Compute the likelihood matrix

   $$
   \mathbf{L}_{ik} = p(\hat{\boldsymbol{\beta}}_i \mid \text{GP}_k),
   $$

   for each eQTL $i$ and each GP component $k$.

2. Estimate the prior mixing weights $\hat{\boldsymbol{\pi}}$ by maximizing the marginal likelihood:

   $$
   \hat{\boldsymbol{\pi}} = \arg\max_{\boldsymbol{\pi}} \sum_{i=1}^{N} \log\left(\sum_{k=0}^{K} \pi_k \mathbf{L}_{ik}\right).
   $$

3. Compute the posterior weights for each eQTL $i$ and GP component $k$:

   $$
   \tilde{\pi}_{ik} = \frac{\pi_k \mathbf{L}_{ik}}{\sum_{j=0}^{K} \pi_j \mathbf{L}_{ij}},
   $$

   where $\tilde{\pi}_{i0}$ is the local false discovery rate (LFDR) under the null hypothesis $\beta_i(t) \in S_0$.

### Testing Nonlinear Dynamic

We first aim to detect dynamic eQTLs with nonlinear dynamics (Category C).

In this case, we specify the base model $S_0 = {\beta(t) = c + bt \mid c, b \in \mathbb{R}}$ as the space of linear functions. Therefore, we choose $L = \frac{d^2}{dt^2}$, which is a second-order differential operator. This choice corresponds to the second Integrated Wiener Process (IWP2) model (`order = 2`).

We aim to obtain the posterior for each $\beta_i(t)$ and test the null hypothesis $H_0: \beta_i(t) \in S_0$ at a given FDR level.

```{r, results="hide"}
fash_fit <- fash(Y = "y", smooth_var = "x", S = "sd", data_list = datasets, order = 2)
```

This is the output of `fash()`:

```{r}
fash_fit
```

Here, the grid of values $\{\sigma_1, \ldots, \sigma_K\}$ is specified using the `grid` argument. Rather than being defined on the original scale, the `grid` is specified on a slightly transformed scale for easier interpretation.

Specifically, given a prediction step size $h$ (`pred_step`), each grid point is defined as $c_h \sigma_k$, where $c_h$ is a positive scaling constant that depends only on $h$ and $L$. This scaling ensures that $c_h \sigma_k$ can be interpreted as $\text{SD}(\beta(t+h) \mid \beta(s), s < t)$, representing the $h$-step predictive standard deviation (PSD).

In the above example, we started with a default grid of 25 equally spaced PSD values from 0 to 2. After empirical Bayes estimation, the prior weights are nonzero only for 5 of these values. Grid values with zero prior weight are considered trivial and are removed automatically.

Let's take a look at the estimated prior $\hat{g}$:

```{r}
fash_fit$prior_weights
```

We can also visualize the fitted prior in FASH by looking at some sample paths. 
As the baseline component is often diffuse, we can visualize the prior sample paths under two different constraints: (1) `initial` constraint, which fixes each sample path (and its derivatives) to have zero initial values; (2) `orthogonal` constraint, which forces each sample path to be orthogonal to the null space $S_0$.

```{r, fig.height=3.5, fig.width=4}
psd_colors <- c("#e41a1c","#377eb8","#4daf4a","#984ea3","#ff7f00")
visualize_fash_prior(fash_fit, constraints = "initial") +
  scale_color_manual(values = psd_colors)
visualize_fash_prior(fash_fit, constraints = "orthogonal") +
  scale_color_manual(values = psd_colors)
```

We can take a look at their posterior weights in each GP component:

```{r, fig.height=4.5, fig.width=3.5}
plot(fash_fit, plot_type = "heatmap",
     selected_indices = c(paste0("A",1:5),
	                      paste0("B",1:5),
						  paste0("C",1:5)))
```

Besides the heatmap plot, we could also visualize the result using a
structure plot:

```{r, fig.height=4, fig.width=5}
plot(fash_fit, plot_type = "structure", discrete = TRUE)
```

We can then use `fdr_control` to test the null hypothesis that $H_0: \beta_i(t) \in S_0$ at a given FDR level.
Specifically, the function takes the local false discovery rate (LFDR) stored in the `fash` object and computes the cumulative false discovery rate (FDR).

```{r, fig.height=4, fig.width=4}
fdr_result <- fdr_control(fash_fit, alpha = 0.1, plot = TRUE)
```

There are `r sum(fdr_result$fdr_results$FDR < 0.1)` eQTLs flagged as significant at FDR level 0.1. We can take out the indices of these eQTLs:

```{r}
detected_indices <- fdr_result$fdr_results$index[fdr_result$fdr_results$FDR < 0.1]
```

How many of the true (non-linear) dynamic eQTLs are detected?

```{r}
sum(labels[detected_indices] == "C")/sizeC
```

What is the false discovery rate?

```{r}
sum(labels[detected_indices] != "C")/length(detected_indices)
```

Let's take a look at the inferred eQTL effect $\beta_i(t)$ for the detected eQTLs. 
The function `predict` computes the posterior distribution of the effect size $\beta_i(t)$ for a given eQTL $i$, and then either returns the posterior summary or posterior samples, depending on the arguments specified.

```{r}
fitted_beta <- predict(fash_fit, index = detected_indices[1])
fitted_beta
```

By default, `predict` returns the posterior information of the effect size $\beta_i(t)$ for the eQTL specified by `index`, at each observed time point.
We can also specify the time points to predict the effect size at:

```{r}
fitted_beta_new <- predict(fash_fit, index = detected_indices[1], smooth_var = seq(0, 16, length.out = 100))
head(fitted_beta_new)
```

It is also possible to store M posterior samples rather than the posterior summary:

```{r}
fitted_beta_samples <- predict(fash_fit, index = detected_indices[1], 
                               smooth_var = seq(0, 16, length.out = 100), 
                               only.samples = TRUE, M = 50)
str(fitted_beta_samples)
```

Let's plot the inferred effect size for the first detected eQTL:

```{r, fig.height=4, fig.width=4}
plot(datasets[[detected_indices[1]]]$x, datasets[[detected_indices[1]]]$y, type = "p", col = "black", lwd = 2, xlab = "Time", ylab = "Effect Size")
lines(fitted_beta_new$x, fitted_beta_new$mean, col = "red", lwd = 2)
lines(datasets[[detected_indices[1]]]$x, datasets[[detected_indices[1]]]$truef, col = "black", lwd = 1, lty = 2)
polygon(c(fitted_beta_new$x, rev(fitted_beta_new$x)), c(fitted_beta_new$lower, rev(fitted_beta_new$upper)), col = rgb(1, 0, 0, 0.2), border = NA)
```

### Testing Dynamic eQTLs

We may also be interested in detecting any dynamic eQTLs, including both linear and nonlinear cases (Categories B and C).

In this case, the base model $S_0 = \{\beta(t) = c \mid c \in \mathbb{R}\}$ represents the space of constant functions. We specify $L = \frac{d}{dt}$, which is a first-order differential operator. 
This choice corresponds to the first Integrated Wiener Process (IWP1) model (`order = 1`).

```{r, results="hide"}
fash_fit_2 <- fash(Y = "y", smooth_var = "x", S = "sd", data_list = datasets, order = 1)
fash_fit_2
```

Take a look at the structure plot:

```{r, fig.height=4, fig.width=4}
plot(fash_fit_2, discrete = TRUE)
```

Let's test the null hypothesis that $H_0: \beta_i(t) \in S_0$ at a
given FDR level (specifying `alpha = 0.1`):

```{r, fig.height=4, fig.width=4}
fdr_result_2 <- fdr_control(fash_fit_2, alpha = 0.1, plot = TRUE)
detected_indices_2 <- fdr_result_2$fdr_results$index[fdr_result_2$fdr_results$FDR < 0.1]
```

How many of the true dynamic eQTLs are detected?

```{r}
sum(labels[detected_indices_2] != "A")/(sizeB + sizeC)
```

What is the false discovery rate?

```{r}
sum(labels[detected_indices_2] == "A")/length(detected_indices_2)
```


